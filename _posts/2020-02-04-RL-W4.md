# \[RL\] [Fundamentals of Reinforcement Learning](https://www.coursera.org/learn/fundamentals-of-reinforcement-learning/home/welcome) [Week 4](https://www.coursera.org/learn/fundamentals-of-reinforcement-learning/home/week/4)

### What is policy evaluation?

Evaluate how good the policy is by evaluating its value functions.

With inputs: policy, find value functions.

### What is control?

Control the policy to maximize value/rewards.

With inputs: dynamic probabilities and value functions, find optimal policy.

### What is dynamic programming?

A way to compute for the optimal policy and value functions which uses bellman equations.

A substitute to the linear equations solver.

### What is iterative policy evaluation?

Evaluate the value functions of a policy iteratively.

### How is iterative policy evaluation done?

Through bellman optimality equation updates. We update your previous value functions according to the newly computed value functions based off of bellman optimality equation.

We can have 2 arrays of value functions: one for the previous and one for the new. We compute the new value functions based on the previous value functions. After computing it for all states, we then update by overwriting the previous value functions with the new ones. This update of all states is called a sweep. When the change between the previous and new value functions becomes sufficiently small, we stop our updates. We conclude that we have reached the optimal/true value function for that policy.

### What is a sweep?

Updating all previous value functions with the newly computed value functions.
